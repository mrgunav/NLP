{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "pAayJMwxMk3k"
   },
   "source": [
    "# Install Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 187
    },
    "colab_type": "code",
    "id": "JWXZ53D5MnRR",
    "outputId": "fabc7d60-7abc-401e-c67c-1e12d4b59088"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: textsearch in /anaconda3/lib/python3.6/site-packages\n",
      "Requirement already satisfied: Unidecode in /anaconda3/lib/python3.6/site-packages (from textsearch)\n",
      "Requirement already satisfied: pyahocorasick in /anaconda3/lib/python3.6/site-packages (from textsearch)\n",
      "\u001b[33mYou are using pip version 9.0.3, however version 19.2.3 is available.\n",
      "You should consider upgrading via the 'pip install --upgrade pip' command.\u001b[0m\n",
      "Requirement already satisfied: contractions in /anaconda3/lib/python3.6/site-packages\n",
      "\u001b[33mYou are using pip version 9.0.3, however version 19.2.3 is available.\n",
      "You should consider upgrading via the 'pip install --upgrade pip' command.\u001b[0m\n",
      "Requirement already satisfied: tqdm in /anaconda3/lib/python3.6/site-packages\n",
      "\u001b[33mYou are using pip version 9.0.3, however version 19.2.3 is available.\n",
      "You should consider upgrading via the 'pip install --upgrade pip' command.\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /Users/mnoordeen/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/mnoordeen/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "!pip install textsearch\n",
    "!pip install contractions\n",
    "!pip install tqdm\n",
    "import nltk\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Uninstalling numpy-1.17.2:\n",
      "\u001b[31mException:\n",
      "Traceback (most recent call last):\n",
      "  File \"/anaconda3/lib/python3.6/shutil.py\", line 544, in move\n",
      "    os.rename(src, real_dst)\n",
      "PermissionError: [Errno 13] Permission denied: '/anaconda3/lib/python3.6/site-packages/numpy-1.17.2.dist-info/INSTALLER' -> '/var/folders/d8/7mbz708s3qx87y491n298f_r0000gp/T/pip-9jin58tw-uninstall/anaconda3/lib/python3.6/site-packages/numpy-1.17.2.dist-info/INSTALLER'\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/anaconda3/lib/python3.6/site-packages/pip/basecommand.py\", line 215, in main\n",
      "    status = self.run(options, args)\n",
      "  File \"/anaconda3/lib/python3.6/site-packages/pip/commands/uninstall.py\", line 76, in run\n",
      "    requirement_set.uninstall(auto_confirm=options.yes)\n",
      "  File \"/anaconda3/lib/python3.6/site-packages/pip/req/req_set.py\", line 346, in uninstall\n",
      "    req.uninstall(auto_confirm=auto_confirm)\n",
      "  File \"/anaconda3/lib/python3.6/site-packages/pip/req/req_install.py\", line 754, in uninstall\n",
      "    paths_to_remove.remove(auto_confirm)\n",
      "  File \"/anaconda3/lib/python3.6/site-packages/pip/req/req_uninstall.py\", line 115, in remove\n",
      "    renames(path, new_path)\n",
      "  File \"/anaconda3/lib/python3.6/site-packages/pip/utils/__init__.py\", line 267, in renames\n",
      "    shutil.move(old, new)\n",
      "  File \"/anaconda3/lib/python3.6/shutil.py\", line 559, in move\n",
      "    os.unlink(src)\n",
      "PermissionError: [Errno 13] Permission denied: '/anaconda3/lib/python3.6/site-packages/numpy-1.17.2.dist-info/INSTALLER'\u001b[0m\n",
      "\u001b[33mYou are using pip version 9.0.3, however version 19.2.3 is available.\n",
      "You should consider upgrading via the 'pip install --upgrade pip' command.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip uninstall numpy --y\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2K\u001b[38;5;2mâœ” Loaded compatibility table\u001b[0m\n",
      "\u001b[1m\n",
      "====================== Installed models (spaCy v2.1.8) ======================\u001b[0m\n",
      "\u001b[38;5;4mâ„¹ spaCy installation: /anaconda3/lib/python3.6/site-packages/spacy\u001b[0m\n",
      "\n",
      "TYPE      NAME                MODEL               VERSION                            \n",
      "package   en-vectors-web-lg   en_vectors_web_lg   \u001b[38;5;2m2.1.0\u001b[0m   \u001b[38;5;2mâœ”\u001b[0m\n",
      "package   en-core-web-sm      en_core_web_sm      \u001b[38;5;2m2.1.0\u001b[0m   \u001b[38;5;2mâœ”\u001b[0m\n",
      "package   en-core-web-md      en_core_web_md      \u001b[38;5;1m2.0.0\u001b[0m   --> 2.1.0     \n",
      "package   en-core-web-lg      en_core_web_lg      \u001b[38;5;1m2.0.0\u001b[0m   --> 2.1.0     \n",
      "link      en_core_web_md      en_core_web_md      \u001b[38;5;1m2.0.0\u001b[0m   --> 2.1.0     \n",
      "link      en_core_web_lg      en_core_web_lg      \u001b[38;5;1m2.0.0\u001b[0m   --> 2.1.0     \n",
      "link      en_core_web_sm      en_core_web_sm      \u001b[38;5;2m2.1.0\u001b[0m   \u001b[38;5;2mâœ”\u001b[0m\n",
      "link      en                  en_core_web_sm      \u001b[38;5;2m2.1.0\u001b[0m   \u001b[38;5;2mâœ”\u001b[0m\n",
      "\n",
      "\u001b[1m\n",
      "============================== Install updates ==============================\u001b[0m\n",
      "Use the following commands to update the model packages:\n",
      "python -m spacy download en_core_web_md\n",
      "python -m spacy download en_core_web_lg\n",
      "\n",
      "You may also want to overwrite the incompatible links using the `python -m spacy\n",
      "link` command with `--force`, or remove them from the data directory. Data path:\n",
      "/anaconda3/lib/python3.6/site-packages/spacy/data\n"
     ]
    }
   ],
   "source": [
    "! python -m spacy validate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: en_core_web_sm==2.1.0 from https://github.com/explosion/spacy-models/releases/download/en_core_web_sm-2.1.0/en_core_web_sm-2.1.0.tar.gz#egg=en_core_web_sm==2.1.0 in /anaconda3/lib/python3.6/site-packages\n",
      "\u001b[33mYou are using pip version 9.0.3, however version 19.2.3 is available.\n",
      "You should consider upgrading via the 'pip install --upgrade pip' command.\u001b[0m\n",
      "\u001b[38;5;2mâœ” Download and installation successful\u001b[0m\n",
      "You can now load the model via spacy.load('en_core_web_sm')\n",
      "\u001b[38;5;2mâœ” Linking successful\u001b[0m\n",
      "/anaconda3/lib/python3.6/site-packages/en_core_web_sm -->\n",
      "/anaconda3/lib/python3.6/site-packages/spacy/data/en\n",
      "You can now load the model via spacy.load('en')\n"
     ]
    }
   ],
   "source": [
    "!python -m spacy download en"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "v0G_9oc6ICdU"
   },
   "outputs": [],
   "source": [
    "import re\n",
    "from bs4 import BeautifulSoup\n",
    "import unicodedata\n",
    "import contractions\n",
    "import spacy\n",
    "import nltk\n",
    "\n",
    "nlp = spacy.load('en')\n",
    "ps = nltk.porter.PorterStemmer()\n",
    "\n",
    "\n",
    "def strip_html_tags(text):\n",
    "    soup = BeautifulSoup(text, \"html.parser\")\n",
    "    [s.extract() for s in soup(['iframe', 'script'])]\n",
    "    stripped_text = soup.get_text()\n",
    "    stripped_text = re.sub(r'[\\r|\\n|\\r\\n]+', '\\n', stripped_text)\n",
    "    return stripped_text\n",
    "\n",
    "\n",
    "def remove_accented_chars(text):\n",
    "    text = unicodedata.normalize('NFKD', text).encode('ascii', 'ignore').decode('utf-8', 'ignore')\n",
    "    return text\n",
    "\n",
    "\n",
    "def expand_contractions(text):\n",
    "    return contractions.fix(text)\n",
    "\n",
    "\n",
    "def spacy_lemmatize_text(text):\n",
    "    text = nlp(text)\n",
    "    text = ' '.join([word.lemma_ if word.lemma_ != '-PRON-' else word.text for word in text])\n",
    "    return text\n",
    "\n",
    "\n",
    "def simple_stemming(text, stemmer=ps):\n",
    "    text = ' '.join([stemmer.stem(word) for word in text.split()])\n",
    "    return text\n",
    "\n",
    "\n",
    "def remove_special_characters(text, remove_digits=False):\n",
    "    pattern = r'[^a-zA-Z0-9\\s]' if not remove_digits else r'[^a-zA-Z\\s]'\n",
    "    text = re.sub(pattern, '', text)\n",
    "    return text\n",
    "\n",
    "\n",
    "def remove_stopwords(text, is_lower_case=False, stopwords=None):\n",
    "    if not stopwords:\n",
    "        stopwords = nltk.corpus.stopwords.words('english')\n",
    "    tokens = nltk.word_tokenize(text)\n",
    "    tokens = [token.strip() for token in tokens]\n",
    "    \n",
    "    if is_lower_case:\n",
    "        filtered_tokens = [token for token in tokens if token not in stopwords]\n",
    "    else:\n",
    "        filtered_tokens = [token for token in tokens if token.lower() not in stopwords]\n",
    "    \n",
    "    filtered_text = ' '.join(filtered_tokens)    \n",
    "    return filtered_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 85
    },
    "colab_type": "code",
    "id": "Aloz8FEcICdY",
    "outputId": "7ec681c7-aff6-4237-97df-20cf7cec1640"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hello\r\n",
      "how are you doing\r\n",
      "I\tam\tdoing\tgreat\r\n",
      ":)\n"
     ]
    }
   ],
   "source": [
    "s = 'hello\\r\\nhow are you doing\\r\\nI\\tam\\tdoing\\tgreat\\r\\n:)'\n",
    "print(s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "9lvBAtV9ICdb",
    "outputId": "176f21c8-5e8f-46a5-fc21-fa548c5e3592"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'hello  how are you doing  I am doing great  :)'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "s.translate(s.maketrans(\"\\n\\t\\r\", \"   \"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'game is on '"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Reference : https://gist.github.com/slowkow/7a7f61f495e3dbb7e3d767f97bd7304b\n",
    "import re\n",
    "def remove_emoji(string):\n",
    "    emoji_pattern = re.compile(\"[\"\n",
    "                           u\"\\U0001F600-\\U0001F64F\"  # emoticons\n",
    "                           u\"\\U0001F300-\\U0001F5FF\"  # symbols & pictographs\n",
    "                           u\"\\U0001F680-\\U0001F6FF\"  # transport & map symbols\n",
    "                           u\"\\U0001F1E0-\\U0001F1FF\"  # flags (iOS)\n",
    "                           u\"\\U00002702-\\U000027B0\"\n",
    "                           u\"\\U000024C2-\\U0001F251\"\n",
    "                           \"]+\", flags=re.UNICODE)\n",
    "    return emoji_pattern.sub(r'', string)\n",
    "\n",
    "remove_emoji(\"game is on ðŸ”¥ðŸ”¥\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already up-to-date: emot in /anaconda3/lib/python3.6/site-packages\n",
      "\u001b[33mYou are using pip version 9.0.3, however version 19.2.3 is available.\n",
      "You should consider upgrading via the 'pip install --upgrade pip' command.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip install emot --upgrade"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from emo_unicode import EMOTICONS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Hello '"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Removal of EmoticonsÂ¶\n",
    "def remove_emoticons(text):\n",
    "    emoticon_pattern = re.compile(u'(' + u'|'.join(k for k in EMOTICONS) + u')')\n",
    "    return emoticon_pattern.sub(r'', text)\n",
    "\n",
    "remove_emoticons(\"Hello :-)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Hello Happy_face_smiley Happy_face_smiley'"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Conversion of Emoticon to Words\n",
    "def convert_emoticons(text):\n",
    "    for emot in EMOTICONS:\n",
    "        text = re.sub(u'('+emot+')', \"_\".join(EMOTICONS[emot].replace(\",\",\"\").split()), text)\n",
    "    return text\n",
    "\n",
    "text = \"Hello :-) :-)\"\n",
    "convert_emoticons(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from emo_unicode import UNICODE_EMO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "':fire:'"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "UNICODE_EMO[\"ðŸ”¥\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'game is on  fire  fire '"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import emoji\n",
    "def convert_emojis(text):\n",
    "    return emoji.demojize(text).replace(\":\",\" \").replace(\",\",\" \")\n",
    "\n",
    "\n",
    "convert_emojis(\"game is on ðŸ”¥ðŸ”¥\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "def translator(user_string):\n",
    "    user_string = user_string.split(\" \")\n",
    "    j = 0\n",
    "    for _str in user_string:\n",
    "        # File path which consists of Abbreviations.\n",
    "        fileName = \"slang.txt\"\n",
    "        # File Access mode [Read Mode]\n",
    "        accessMode = \"r\"\n",
    "        with open(fileName, accessMode) as myCSVfile:\n",
    "            # Reading file as CSV with delimiter as \"=\", so that abbreviation are stored in row[0] and phrases in row[1]\n",
    "            dataFromFile = csv.reader(myCSVfile, delimiter=\"=\")\n",
    "            # Removing Special Characters.\n",
    "            _str = re.sub('[^a-zA-Z0-9-_.]', '', _str)\n",
    "            for row in dataFromFile:\n",
    "                # Check if selected word matches short forms[LHS] in text file.\n",
    "                if _str.upper() == row[0]:\n",
    "                    # If match found replace it with its Abbreviation in text file.\n",
    "                    user_string[j] = row[1]\n",
    "            myCSVfile.close()\n",
    "        j = j + 1\n",
    "    # Replacing commas with spaces for final output.\n",
    "    print(' '.join(user_string))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "one minute Be Right Back\n"
     ]
    }
   ],
   "source": [
    "translator(\"one minute BRB\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "34cnm0OkICdd"
   },
   "source": [
    "## Your Turn: Add in all the necessary functions and build your pre-processor!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "_N5OiA07ICdd"
   },
   "outputs": [],
   "source": [
    "import tqdm\n",
    "\n",
    "def text_pre_processor(text, html_strip=True, accented_char_removal=True, contraction_expansion=True,\n",
    "                       text_lower_case=True, text_stemming=False, text_lemmatization=True, \n",
    "                       special_char_removal=True, remove_digits=True, stopword_removal=True, \n",
    "                       stopword_list=None,emoji_remove = False, emoticons_remove=False,emoticons_convert=False,\n",
    "                       translator=False,convert_emojis=False):\n",
    "    \n",
    "    # strip HTML\n",
    "    if html_strip:\n",
    "        text = strip_html_tags(text)\n",
    "    \n",
    "    # remove extra newlines (often might be present in really noisy text)\n",
    "    text = text.translate(text.maketrans(\"\\n\\t\\r\", \"   \"))\n",
    "    \n",
    "    # remove accented characters\n",
    "    if accented_char_removal:\n",
    "        text = remove_accented_chars(text)\n",
    "    \n",
    "    # expand contractions    \n",
    "    if contraction_expansion:\n",
    "        text = expand_contractions(text)\n",
    "        \n",
    "    \n",
    "    # lemmatize text\n",
    "    if text_lemmatization:\n",
    "        text = spacy_lemmatize_text(text) \n",
    "        \n",
    "    # remove special characters and\\or digits    \n",
    "    if special_char_removal:\n",
    "        # insert spaces between special characters to isolate them    \n",
    "        special_char_pattern = re.compile(r'([{.(-)!}])')\n",
    "        text = special_char_pattern.sub(\" \\\\1 \", text)\n",
    "        text = remove_special_characters(text, remove_digits=remove_digits)  \n",
    "        \n",
    "    # stem text\n",
    "    if text_stemming and not text_lemmatization:\n",
    "        text = simple_stemming(text)\n",
    "        \n",
    "    # lowercase the text    \n",
    "    if text_lower_case:\n",
    "        text = text.lower()\n",
    "        \n",
    "        \n",
    "    # remove stopwords\n",
    "    if stopword_removal:\n",
    "        text = remove_stopwords(text, is_lower_case=text_lower_case, \n",
    "                                stopwords=stopword_list)\n",
    "    # remove emoji  \n",
    "    if emoji_remove:\n",
    "        text = remove_emoji(text)\n",
    "        \n",
    "    #remove_emoticons\n",
    "    if emoticons_remove:\n",
    "        text = remove_emoticons(text)  \n",
    "        \n",
    "    #convert_emoticons\n",
    "    if emoticons_convert:\n",
    "        text = convert_emoticons(text)\n",
    "        \n",
    "    #convert emojis\n",
    "    if convert_emojis:\n",
    "        text = convert_emojis(text)\n",
    "        \n",
    "    if translator:\n",
    "        text = translator(text)\n",
    "    \n",
    "    # remove extra whitespace\n",
    "    text = re.sub(' +', ' ', text)\n",
    "    text = text.strip()\n",
    "    \n",
    "    \n",
    "    return text\n",
    "\n",
    "  \n",
    "def corpus_pre_processor(corpus):\n",
    "  norm_corpus = []\n",
    "  for doc in tqdm.tqdm(corpus):\n",
    "    norm_corpus.append(text_pre_processor(doc))\n",
    "  return norm_corpus"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "nBE3P53iICdg"
   },
   "source": [
    "# Test on a single document"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 221
    },
    "colab_type": "code",
    "id": "CjL4Mx9aICdg",
    "outputId": "0525fb8c-914e-4537-a7c1-97173266708b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<p>HÃ©llo! HÃ©llo! can you hear me! I just heard about <b>Python</b>!<br/>\r\n",
      " \n",
      "              It's an amazing language which can be used for [Scripting\tWeb development\tBackend development],\r\n",
      "\r\n",
      "\n",
      "              Information Retrieval, Natural Language Processing, Machine Learning & Artificial Intelligence!\n",
      "\n",
      "              What are you waiting for? Go and get started.<br/> He's learning, she's learning, they've already\n",
      "\n",
      "\n",
      "              got a headstart! GET PYTHON 3.6 NOW!</p>\n",
      "           \n"
     ]
    }
   ],
   "source": [
    "document = \"\"\"<p>HÃ©llo! HÃ©llo! can you hear me! I just heard about <b>Python</b>!<br/>\\r\\n \n",
    "              It's an amazing language which can be used for [Scripting\\tWeb development\\tBackend development],\\r\\n\\r\\n\n",
    "              Information Retrieval, Natural Language Processing, Machine Learning & Artificial Intelligence!\\n\n",
    "              What are you waiting for? Go and get started.<br/> He's learning, she's learning, they've already\\n\\n\n",
    "              got a headstart! GET PYTHON 3.6 NOW!</p>\n",
    "           \"\"\"\n",
    "print(document)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 54
    },
    "colab_type": "code",
    "id": "q67MCFxLICdj",
    "outputId": "84033398-3704-4f6a-bfb2-edd3a34b0299"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'hello hello hear hear python amazing language use scripting web development backend development information retrieval natural language processing machine learning artificial intelligence wait go get start learn learn already get headstart get python'"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text_pre_processor(document)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "HsIpekwoICdl"
   },
   "source": [
    "# Test on a corpus of documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "2S1H1zakICdo"
   },
   "outputs": [],
   "source": [
    "corpus = [\"\"\"<p>HÃ©llo! HÃ©llo! can you hear me! I just heard about <b>Python</b>!<br/>\\r\\n \n",
    "              It's an amazing language which can be used for [Scripting\\tWeb development\\tBackend development],\\r\\n\\r\\n\n",
    "              Information Retrieval, Natural Language Processing, Machine Learning & Artificial Intelligence!\\n\n",
    "              What are you waiting for? Go and get started.<br/> He's learning, she's learning, they've already\\n\\n\n",
    "              got a headstart! GET PYTHON 3.6 NOW!</p>\n",
    "           \"\"\",\n",
    "          \"\"\"US unveils world's most powerful supercomputer, beats China. \n",
    "             The US has unveiled the world's most powerful supercomputer \n",
    "             called 'Summit', beating the previous record-holder China's Sunway \n",
    "             TaihuLight. With a peak performance of 200,000 trillion calculations \n",
    "             per second, it is over twice as fast as Sunway TaihuLight, which is capable \n",
    "             of 93,000 trillion calculations per second. Summit has 4,608 servers, \n",
    "             which reportedly take up the size of two tennis courts.\"\"\",\n",
    "          \"\"\"The Lord of the Rings is an epic high fantasy novel written by English author and scholar J. R. R. Tolkien. \n",
    "            The story began as a sequel to Tolkien's 1937 fantasy novel The Hobbit, but eventually developed into \n",
    "            a much larger work. Written in stages between 1937 and 1949, The Lord of the Rings is one of the \n",
    "            best-selling novels ever written, with over 150 million copies sold.[1]\n",
    "          \"\"\",\n",
    "          \"\"\"The title of the novel refers to the story's main antagonist, the Dark Lord Sauron,[a] \n",
    "             who had in an earlier age created the One Ring to rule the other Rings of Power as the ultimate weapon \n",
    "             in his campaign to conquer and rule all of Middle-earth. From quiet beginnings in the Shire, a hobbit \n",
    "             land not unlike the English countryside, the story ranges across Middle-earth, following the course \n",
    "             of the War of the Ring through the eyes of its characters, not only the hobbits Frodo Baggins, \n",
    "             Samwise \"Sam\" Gamgee, Meriadoc \"Merry\" Brandybuck and Peregrin \"Pippin\" Took, but also the hobbits' \n",
    "             chief allies and travelling companions: the Men, Aragorn, a Ranger of the North, and Boromir, \n",
    "             a Captain of Gondor; Gimli, a Dwarf warrior; Legolas Greenleaf, an Elven prince; and Gandalf, a wizard.\n",
    "          \"\"\"\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 88
    },
    "colab_type": "code",
    "id": "rj7oRS8hQYco",
    "outputId": "da2bc31f-b14f-4c2a-edac-6ee6c005faa1"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 4/4 [00:00<00:00, 37.44it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['hello hello hear hear python amazing language use scripting web development backend development information retrieval natural language processing machine learning artificial intelligence wait go get start learn learn already get headstart get python',\n",
       " 'us unveil world powerful supercomputer beat china us unveil world powerful supercomputer call summit beat previous record holder china sunway taihulight peak performance trillion calculation per second twice fast sunway taihulight capable trillion calculation per second summit server reportedly take size two tennis court']"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "norm_docs = corpus_pre_processor(corpus)\n",
    "norm_docs[:2]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "jZRQDfTTQpzh"
   },
   "source": [
    "# Processor with multi-threading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "fyYMPuirQbj6"
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "from concurrent import futures\n",
    "import threading\n",
    "\n",
    "def parallel_preprocessing(idx, doc, total_docs):\n",
    "    return text_pre_processor(doc)\n",
    "\n",
    "\n",
    "def pre_process_documents_parallel(documents):\n",
    "    total_docs = len(documents)\n",
    "    docs_input = [[idx, doc, total_docs] for idx, doc in enumerate(documents)]\n",
    "    \n",
    "    ex = futures.ThreadPoolExecutor(max_workers=None)\n",
    "    print('preprocessing: starting')\n",
    "    norm_descriptions_map = ex.map(parallel_preprocessing, \n",
    "                                   [record[0] for record in docs_input],\n",
    "                                   [record[1] for record in docs_input],\n",
    "                                   [record[2] for record in docs_input])\n",
    "    print(\"preprocessing: end\")\n",
    "    norm_descriptions = list(norm_descriptions_map)\n",
    "    return norm_descriptions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 122
    },
    "colab_type": "code",
    "id": "HFBm2MmZRck3",
    "outputId": "fd917fea-0e31-46e5-af2d-2ccbea81a7f3"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "preprocessing: starting\n",
      "preprocessing: end\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['hello hello hear hear python amazing language use scripting web development backend development information retrieval natural language processing machine learning artificial intelligence wait go get start learn learn already get headstart get python',\n",
       " 'us unveil world powerful supercomputer beat china us unveil world powerful supercomputer call summit beat previous record holder china sunway taihulight peak performance trillion calculation per second twice fast sunway taihulight capable trillion calculation per second summit server reportedly take size two tennis court',\n",
       " 'lord rings epic high fantasy novel write english author scholar j r r tolkien story begin sequel tolkien fantasy novel hobbit eventually develop much large work write stage lord rings one best sell novel ever write million copy sold',\n",
       " 'title novel refer story main antagonist dark lord saurona early age create one ring rule rings power ultimate weapon campaign conquer rule middle earth quiet beginning shire hobbit land unlike english countryside story range across middle earth follow course war ring eye character hobbit frodo baggins samwise sam gamgee meriadoc merry brandybuck peregrin pippin take also hobbit chief ally travel companion men aragorn ranger north boromir captain gondor gimli dwarf warrior legolas greenleaf elven prince gandalf wizard']"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "norm_docs = pre_process_documents_parallel(corpus)\n",
    "norm_docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "cDihxH-bZwsU"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "Project - Build your Text Pre-processor.ipynb",
   "provenance": [],
   "toc_visible": true,
   "version": "0.3.2"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
